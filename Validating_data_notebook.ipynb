{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5371cda3",
   "metadata": {},
   "source": [
    "# Validating data notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f418367",
   "metadata": {},
   "source": [
    "#### In this notebook is a demonesrtaion of how to validate data using another data source\n",
    "\n",
    "This is done using a data `pipeline` that will ingest and clean our data with the press of a button"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88e725a",
   "metadata": {},
   "source": [
    "we will use a dataset from an imaginary country called `Maji Ndogo` created by `Explore-AI` academy\n",
    "the dataset is containing agricultural data about the fields of `Maji Ndogo`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc719458",
   "metadata": {},
   "source": [
    "the dataset contains a wheater data and we will validate these data using a data collected from a weather stations in the area of each field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1548521f",
   "metadata": {},
   "source": [
    "Our main goal is: Is the data in our `MD_agric_df` dataset representative of reality? To answer this, we use weather-related data from nearby stations to validate our results. If the weather data matches the data we have, we can be more confident that our dataset represents reality. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607ba18a",
   "metadata": {},
   "source": [
    "So what's the plan? \n",
    "1. Create a null hypothesis.\n",
    "1. Import the `MD_agric_df` dataset and clean it up.\n",
    "1. Import the weather data.\n",
    "1. Map the weather data to the field data.\n",
    "1. Calculate the means of the weather station dataset and the means of the main dataset.\n",
    "2. Calculate all the parameters we need to do a t-test. \n",
    "3. Interpret our results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c47bb5",
   "metadata": {},
   "source": [
    "# Data dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f3cfa1",
   "metadata": {},
   "source": [
    "**1. Geographic features**\n",
    "\n",
    "- **Field_ID:** A unique identifier for each field (BigInt).\n",
    " \n",
    "- **Elevation:** The elevation of the field above sea level in metres (Float).\n",
    "\n",
    "- **Latitude:** Geographical latitude of the field in degrees (Float).\n",
    "\n",
    "- **Longitude:** Geographical longitude of the field in degrees (Float).\n",
    "\n",
    "- **Location:** Province the field is in (Text).\n",
    "\n",
    "- **Slope:** The slope of the land in the field (Float).\n",
    "\n",
    "**2. Weather features**\n",
    "\n",
    "- **Field_ID:** Corresponding field identifier (BigInt).\n",
    "\n",
    "- **Rainfall:** Amount of rainfall in the area in mm (Float).\n",
    "\n",
    "- **Min_temperature_C:** Average minimum temperature recorded in Celsius (Float).\n",
    "\n",
    "- **Max_temperature_C:** Average maximum temperature recorded in Celsius (Float).\n",
    "\n",
    "- **Ave_temps:** Average temperature in Celcius (Float).\n",
    "\n",
    "**3. Soil and crop features**\n",
    "\n",
    "- **Field_ID:** Corresponding field identifier (BigInt).\n",
    "\n",
    "- **Soil_fertility:** A measure of soil fertility where 0 is infertile soil, and 1 is very fertile soil (Float).\n",
    "\n",
    "- **Soil_type:** Type of soil present in the field (Text).\n",
    "\n",
    "- **pH:** pH level of the soil, which is a measure of how acidic/basic the soil is (Float).\n",
    "\n",
    "**4. Farm management features**\n",
    "\n",
    "- **Field_ID:** Corresponding field identifier (BigInt).\n",
    "\n",
    "- **Pollution_level:** Level of pollution in the area where 0 is unpolluted and 1 is very polluted (Float).\n",
    "\n",
    "- **Plot_size:** Size of the plot in the field (Ha) (Float).\n",
    "\n",
    "- **Chosen_crop:** Type of crop chosen for cultivation (Text).\n",
    "\n",
    "- **Annual_yield:** Annual yield from the field (Float). This is the total output of the field. The field size and type of crop will affect the Annual Yield\n",
    "\n",
    "- **Standard_yield:** Standardised yield expected from the field, normalised per crop (Float). This is independent of field size, or crop type. Multiplying this number by the field size, and average crop yield will give the Annual_Yield.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Weather_station_data (CSV)**\n",
    "\n",
    "- **Weather_station_ID:** The weather station the data originated from. (Int)\n",
    "\n",
    "- **Message:** The weather data was captured by sensors at the stations, in the format of text messages.(Str)\n",
    "\n",
    "**Weather_data_field_mapping (CSV)**\n",
    "\n",
    "- **Field_ID:** The id of the field that is connected to a weather station. This is the key we can use to join the weather station ID to the original data. (Int)\n",
    "\n",
    "- **Weather_station_ID:** The weather station that is connected to a field. If a field has `weather_station_ID = 0` then that field is closest to weather station 0. (Int)\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7413a4",
   "metadata": {},
   "source": [
    "# Dealing with a friendly warning from Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6862b309",
   "metadata": {},
   "source": [
    "If you are running this notebook in `Python 3.12` or later, you might get a warning if you run the imports below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ff4521",
   "metadata": {},
   "source": [
    "If you are lucky enough to see this warning, it let's us know that Pandas is changing soon and will require another package to be installed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b6ba10",
   "metadata": {},
   "source": [
    "```python\n",
    "...2334042735.py:3: DeprecationWarning: \n",
    "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
    "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
    "but was not found to be installed on your system.\n",
    "If this would cause problems for you,\n",
    "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
    "        \n",
    "  import pandas as pd\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f0febb",
   "metadata": {},
   "source": [
    "We can safely ignore these warnings, but soon our script will fail to import Pandas, so let's fix it today, and we won't have to worry about it for a long time. The warning tells us that Pyarrow will soon be a requirement to import Pandas, so we can just install it with pip. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064d9f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install Pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b715617",
   "metadata": {},
   "source": [
    "# Creating up our data pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a427b29",
   "metadata": {},
   "source": [
    "So here's the plan: \n",
    "\n",
    "1. Create a module for each step.\n",
    "\n",
    "2. We will create three modules: \n",
    "\n",
    "    a. `data_ingesation.py` - All SQL-related functions, and web-based data retrieval.\n",
    "\n",
    "    b. `field_data_processor.py` - All transformations, cleanup, and merging functionality.\n",
    "\n",
    "    c. `weather_data_processor.py` - All transformations and cleanup of the weather station data.\n",
    "\n",
    "3. Test the modules functionality.\n",
    "\n",
    "4. Create automated data validation tests to ensure our data is as we expect it to be.\n",
    "\n",
    "Once we're done with that, we're going to jump into the reason why we're here. So let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffa809d",
   "metadata": {},
   "source": [
    "The first challenge; automating the data ingestion. There are two places we're fetching data:\n",
    "1. SQLite database - We need to create an SQLite engine, connect to the database, run a query and return a pandas DataFrame.\n",
    "2. Web CSV file - Read the CSV data from the web, and import it as a DataFrame.\n",
    "\n",
    "So let's start building!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3401e792",
   "metadata": {},
   "source": [
    "## Data ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646f0f29",
   "metadata": {},
   "source": [
    "Creating modules in Jupyter notebooks is a bit of a pain. If we make changes to the `module.py` file, we have to restart the notebook kernel and import the module again in order to apply those changes. So, we're going to fully develop it, test it in this notebook, and only then, move it to a `data_ingestion.py` file, and import it. \n",
    "\n",
    "To create a module, our code should ideally be encapsulated in functions or classes. How do we choose?\n",
    "\n",
    "Since the process of connecting to a database, and querying some data is relatively straightforward, functions seemed like the best fit. Functions allow us to encapsulate the necessary steps in clear, reusable blocks of code without the overhead of managing class objects. It's like using a **simple tool** for a specific task — pick it up, use it, and put it back without needing to remember anything about the last use.\n",
    "\n",
    "On the other hand, our **data processing** modules are a bit more complex. These modules not only perform various operations on the data but also need to keep track of the data as it goes through these processes. For this, we use **classes** to **create DataFrame objects** as attributes. This approach simplifies data handling since we're passing the object around, which inherently knows its data and the operations it can perform within the class.\n",
    "\n",
    "This idea ties back to OOP. Class objects are designed to deal with data and operations on that data via methods, while functions are made to do the simpler tasks.\n",
    "\n",
    "So for **data ingestion**, we're just going to use functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4143f6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ce5c540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_db_engine(db_path):\n",
    "    engine = create_engine(db_path)\n",
    "    return engine\n",
    "\n",
    "def query_data(engine, sql_query):\n",
    "    with engine.connect() as connection:\n",
    "        df = pd.read_sql_query(text(sql_query), connection)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb325c6",
   "metadata": {},
   "source": [
    "# Here is an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24a35ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Field_ID</th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Location</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Min_temperature_C</th>\n",
       "      <th>Max_temperature_C</th>\n",
       "      <th>Ave_temps</th>\n",
       "      <th>Soil_fertility</th>\n",
       "      <th>Soil_type</th>\n",
       "      <th>pH</th>\n",
       "      <th>Pollution_level</th>\n",
       "      <th>Plot_size</th>\n",
       "      <th>Crop_type</th>\n",
       "      <th>Annual_yield</th>\n",
       "      <th>Standard_yield</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40734</td>\n",
       "      <td>786.05580</td>\n",
       "      <td>-7.389911</td>\n",
       "      <td>-7.556202</td>\n",
       "      <td>Rural_Akatsi</td>\n",
       "      <td>14.795113</td>\n",
       "      <td>1125.2</td>\n",
       "      <td>-3.1</td>\n",
       "      <td>33.1</td>\n",
       "      <td>15.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>Sandy</td>\n",
       "      <td>6.169393</td>\n",
       "      <td>8.526684e-02</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.751354</td>\n",
       "      <td>cassava</td>\n",
       "      <td>0.577964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30629</td>\n",
       "      <td>674.33410</td>\n",
       "      <td>-7.736849</td>\n",
       "      <td>-1.051539</td>\n",
       "      <td>Rural_Sokoto</td>\n",
       "      <td>11.374611</td>\n",
       "      <td>1450.7</td>\n",
       "      <td>-3.9</td>\n",
       "      <td>30.6</td>\n",
       "      <td>13.35</td>\n",
       "      <td>0.64</td>\n",
       "      <td>Volcanic</td>\n",
       "      <td>5.676648</td>\n",
       "      <td>3.996838e-01</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.069865</td>\n",
       "      <td>cassava</td>\n",
       "      <td>0.486302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39924</td>\n",
       "      <td>826.53390</td>\n",
       "      <td>-9.926616</td>\n",
       "      <td>0.115156</td>\n",
       "      <td>Rural_Sokoto</td>\n",
       "      <td>11.339692</td>\n",
       "      <td>2208.9</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>28.4</td>\n",
       "      <td>13.30</td>\n",
       "      <td>0.69</td>\n",
       "      <td>Volcanic</td>\n",
       "      <td>5.331993</td>\n",
       "      <td>3.580286e-01</td>\n",
       "      <td>3.4</td>\n",
       "      <td>2.208801</td>\n",
       "      <td>tea</td>\n",
       "      <td>0.649647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5754</td>\n",
       "      <td>574.94617</td>\n",
       "      <td>-2.420131</td>\n",
       "      <td>-6.592215</td>\n",
       "      <td>Rural_Kilimani</td>\n",
       "      <td>7.109855</td>\n",
       "      <td>328.8</td>\n",
       "      <td>-5.8</td>\n",
       "      <td>32.2</td>\n",
       "      <td>13.20</td>\n",
       "      <td>0.54</td>\n",
       "      <td>Loamy</td>\n",
       "      <td>5.328150</td>\n",
       "      <td>2.866871e-01</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.277635</td>\n",
       "      <td>cassava</td>\n",
       "      <td>0.532348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14146</td>\n",
       "      <td>886.35300</td>\n",
       "      <td>-3.055434</td>\n",
       "      <td>-7.952609</td>\n",
       "      <td>Rural_Kilimani</td>\n",
       "      <td>55.007656</td>\n",
       "      <td>785.2</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>31.0</td>\n",
       "      <td>14.25</td>\n",
       "      <td>0.72</td>\n",
       "      <td>Sandy</td>\n",
       "      <td>5.721234</td>\n",
       "      <td>4.319027e-02</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.832614</td>\n",
       "      <td>wheat</td>\n",
       "      <td>0.555076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5649</th>\n",
       "      <td>11472</td>\n",
       "      <td>681.36145</td>\n",
       "      <td>-7.358371</td>\n",
       "      <td>-6.254369</td>\n",
       "      <td>Rural_Akatsi</td>\n",
       "      <td>16.213196</td>\n",
       "      <td>885.7</td>\n",
       "      <td>-4.3</td>\n",
       "      <td>33.4</td>\n",
       "      <td>14.55</td>\n",
       "      <td>0.61</td>\n",
       "      <td>Sandy</td>\n",
       "      <td>5.741063</td>\n",
       "      <td>3.286828e-01</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.609930</td>\n",
       "      <td>potato</td>\n",
       "      <td>0.554482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5650</th>\n",
       "      <td>19660</td>\n",
       "      <td>667.02120</td>\n",
       "      <td>-3.154559</td>\n",
       "      <td>-4.475046</td>\n",
       "      <td>Rural_Kilimani</td>\n",
       "      <td>2.397553</td>\n",
       "      <td>501.1</td>\n",
       "      <td>-4.8</td>\n",
       "      <td>32.1</td>\n",
       "      <td>13.65</td>\n",
       "      <td>0.54</td>\n",
       "      <td>Sandy</td>\n",
       "      <td>5.445833</td>\n",
       "      <td>1.602583e-01</td>\n",
       "      <td>8.7</td>\n",
       "      <td>3.812289</td>\n",
       "      <td>maize</td>\n",
       "      <td>0.438194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5651</th>\n",
       "      <td>41296</td>\n",
       "      <td>670.77900</td>\n",
       "      <td>-14.472861</td>\n",
       "      <td>-6.110221</td>\n",
       "      <td>Rural_Hawassa</td>\n",
       "      <td>7.636470</td>\n",
       "      <td>1586.6</td>\n",
       "      <td>-3.8</td>\n",
       "      <td>33.4</td>\n",
       "      <td>14.80</td>\n",
       "      <td>0.64</td>\n",
       "      <td>Volcanic</td>\n",
       "      <td>5.385873</td>\n",
       "      <td>8.221326e-09</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1.681629</td>\n",
       "      <td>tea</td>\n",
       "      <td>0.800776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5652</th>\n",
       "      <td>33090</td>\n",
       "      <td>429.48840</td>\n",
       "      <td>-14.653089</td>\n",
       "      <td>-6.984116</td>\n",
       "      <td>Rural_Hawassa</td>\n",
       "      <td>13.944720</td>\n",
       "      <td>1272.2</td>\n",
       "      <td>-6.2</td>\n",
       "      <td>34.6</td>\n",
       "      <td>14.20</td>\n",
       "      <td>0.63</td>\n",
       "      <td>Silt</td>\n",
       "      <td>5.562508</td>\n",
       "      <td>6.917245e-10</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.659874</td>\n",
       "      <td>cassava</td>\n",
       "      <td>0.507595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5653</th>\n",
       "      <td>8375</td>\n",
       "      <td>763.09030</td>\n",
       "      <td>-4.317028</td>\n",
       "      <td>-6.344461</td>\n",
       "      <td>Rural_Kilimani</td>\n",
       "      <td>35.189430</td>\n",
       "      <td>516.4</td>\n",
       "      <td>-3.8</td>\n",
       "      <td>29.6</td>\n",
       "      <td>12.90</td>\n",
       "      <td>0.64</td>\n",
       "      <td>Sandy</td>\n",
       "      <td>5.087792</td>\n",
       "      <td>2.612715e-01</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.226532</td>\n",
       "      <td>wheat</td>\n",
       "      <td>0.453064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5654 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Field_ID  Elevation   Latitude  Longitude        Location      Slope  \\\n",
       "0        40734  786.05580  -7.389911  -7.556202    Rural_Akatsi  14.795113   \n",
       "1        30629  674.33410  -7.736849  -1.051539    Rural_Sokoto  11.374611   \n",
       "2        39924  826.53390  -9.926616   0.115156    Rural_Sokoto  11.339692   \n",
       "3         5754  574.94617  -2.420131  -6.592215  Rural_Kilimani   7.109855   \n",
       "4        14146  886.35300  -3.055434  -7.952609  Rural_Kilimani  55.007656   \n",
       "...        ...        ...        ...        ...             ...        ...   \n",
       "5649     11472  681.36145  -7.358371  -6.254369    Rural_Akatsi  16.213196   \n",
       "5650     19660  667.02120  -3.154559  -4.475046  Rural_Kilimani   2.397553   \n",
       "5651     41296  670.77900 -14.472861  -6.110221   Rural_Hawassa   7.636470   \n",
       "5652     33090  429.48840 -14.653089  -6.984116   Rural_Hawassa  13.944720   \n",
       "5653      8375  763.09030  -4.317028  -6.344461  Rural_Kilimani  35.189430   \n",
       "\n",
       "      Rainfall  Min_temperature_C  Max_temperature_C  Ave_temps  \\\n",
       "0       1125.2               -3.1               33.1      15.00   \n",
       "1       1450.7               -3.9               30.6      13.35   \n",
       "2       2208.9               -1.8               28.4      13.30   \n",
       "3        328.8               -5.8               32.2      13.20   \n",
       "4        785.2               -2.5               31.0      14.25   \n",
       "...        ...                ...                ...        ...   \n",
       "5649     885.7               -4.3               33.4      14.55   \n",
       "5650     501.1               -4.8               32.1      13.65   \n",
       "5651    1586.6               -3.8               33.4      14.80   \n",
       "5652    1272.2               -6.2               34.6      14.20   \n",
       "5653     516.4               -3.8               29.6      12.90   \n",
       "\n",
       "      Soil_fertility Soil_type        pH  Pollution_level  Plot_size  \\\n",
       "0               0.62     Sandy  6.169393     8.526684e-02        1.3   \n",
       "1               0.64  Volcanic  5.676648     3.996838e-01        2.2   \n",
       "2               0.69  Volcanic  5.331993     3.580286e-01        3.4   \n",
       "3               0.54     Loamy  5.328150     2.866871e-01        2.4   \n",
       "4               0.72     Sandy  5.721234     4.319027e-02        1.5   \n",
       "...              ...       ...       ...              ...        ...   \n",
       "5649            0.61     Sandy  5.741063     3.286828e-01        1.1   \n",
       "5650            0.54     Sandy  5.445833     1.602583e-01        8.7   \n",
       "5651            0.64  Volcanic  5.385873     8.221326e-09        2.1   \n",
       "5652            0.63      Silt  5.562508     6.917245e-10        1.3   \n",
       "5653            0.64     Sandy  5.087792     2.612715e-01        0.5   \n",
       "\n",
       "      Crop_type Annual_yield  Standard_yield  \n",
       "0      0.751354      cassava        0.577964  \n",
       "1      1.069865      cassava        0.486302  \n",
       "2      2.208801          tea        0.649647  \n",
       "3      1.277635      cassava        0.532348  \n",
       "4      0.832614        wheat        0.555076  \n",
       "...         ...          ...             ...  \n",
       "5649   0.609930       potato        0.554482  \n",
       "5650   3.812289        maize        0.438194  \n",
       "5651   1.681629          tea        0.800776  \n",
       "5652   0.659874      cassava        0.507595  \n",
       "5653   0.226532        wheat        0.453064  \n",
       "\n",
       "[5654 rows x 18 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SQL_engine = create_db_engine('sqlite:///Maji_Ndogo_farm_survey_small.db')\n",
    "\n",
    "sql_query = \"\"\"\n",
    "SELECT *\n",
    "FROM geographic_features\n",
    "LEFT JOIN weather_features USING (Field_ID)\n",
    "LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "LEFT JOIN farm_management_features USING (Field_ID)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "df = query_data(SQL_engine, sql_query)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e90ea3c",
   "metadata": {},
   "source": [
    "This seems simple, but let's think for a second about what could go wrong. For example, what if there is a problem like the database has a schema, and no actual data? Or our query doesn't return any data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9e6a6e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Field_ID</th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Location</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Min_temperature_C</th>\n",
       "      <th>Max_temperature_C</th>\n",
       "      <th>Ave_temps</th>\n",
       "      <th>Soil_fertility</th>\n",
       "      <th>Soil_type</th>\n",
       "      <th>pH</th>\n",
       "      <th>Pollution_level</th>\n",
       "      <th>Plot_size</th>\n",
       "      <th>Crop_type</th>\n",
       "      <th>Annual_yield</th>\n",
       "      <th>Standard_yield</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Field_ID, Elevation, Latitude, Longitude, Location, Slope, Rainfall, Min_temperature_C, Max_temperature_C, Ave_temps, Soil_fertility, Soil_type, pH, Pollution_level, Plot_size, Crop_type, Annual_yield, Standard_yield]\n",
       "Index: []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SQL_engine = create_db_engine('sqlite:///Maji_Ndogo_farm_survey_small.db')\n",
    "\n",
    "sql_query = \"\"\"\n",
    "SELECT *\n",
    "FROM geographic_features\n",
    "LEFT JOIN weather_features USING (Field_ID)\n",
    "LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "LEFT JOIN farm_management_features USING (Field_ID)\n",
    "WHERE Rainfall < 0 \n",
    "\"\"\"\n",
    "# The last line won't ever be true, so no results will be returned. \n",
    "\n",
    "df = query_data(SQL_engine, sql_query)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ffe6d1",
   "metadata": {},
   "source": [
    "We get an empty DataFrame, because SQL returned an empty query result. When we try to filter results, we get an answer that would not make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d7cbf2",
   "metadata": {},
   "source": [
    "So to avoid this we need to add error handling into our code so that we stop the process if something is wrong, and tell us what the problem is before we continue. \n",
    "\n",
    "Secondly, to help us understand how our code is executing we're going to add some logs. While print statements can help us to debug our code, we have to remove them once our code goes into use, one by one. `logging` is a better way to debug our code than print statements because we can add `logging.INFO()` logs to know what our code is doing, and `logging.DEBUG()` statements that have more detail in case we want to debug a specific loop in a bit more in detail. There are also various other tools to use, and we can also silence all logging with a single line of code. If we used print statements, we will have to comment them out one by one. \n",
    "\n",
    "If we apply these two ideas, we get the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b83ffe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "# Name our logger so we know that logs from this module come from the data_ingestion module\n",
    "logger = logging.getLogger('data_ingestion')\n",
    "\n",
    "# Set a basic logging message up that prints out a timestamp, the name of our logger, and the message\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "def create_db_engine(db_path):\n",
    "    try:\n",
    "        engine = create_engine(db_path)\n",
    "        # Test connection\n",
    "        with engine.connect() as conn:\n",
    "            pass\n",
    "        # test if the database engine was created successfully\n",
    "        logger.info(\"Database engine created successfully.\")\n",
    "        return engine # Return the engine object if it all works well\n",
    "    except ImportError: #If we get an ImportError, inform the user SQLAlchemy is not installed\n",
    "        logger.error(\"SQLAlchemy is required to use this function. Please install it first.\")\n",
    "        raise e\n",
    "    except Exception as e:# If we fail to create an engine inform the user\n",
    "        logger.error(f\"Failed to create database engine. Error: {e}\")\n",
    "        raise e\n",
    "    \n",
    "def query_data(engine, sql_query):\n",
    "    try:\n",
    "        with engine.connect() as connection:\n",
    "            df = pd.read_sql_query(text(sql_query), connection)\n",
    "        if df.empty:\n",
    "            # Log a message or handle the empty DataFrame scenario as needed\n",
    "            logger.error(\"The query returned an empty DataFrame.\")\n",
    "            raise ValueError(\"The query returned an empty DataFrame.\")\n",
    "        logger.info(\"Query executed successfully.\")\n",
    "        return df\n",
    "    except ValueError as e: \n",
    "        logger.error(f\"SQL query failed. Error: {e}\")\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred while querying the database. Error: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1ee942",
   "metadata": {},
   "source": [
    "Now when we run the incorrect query again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb6f078b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-01 18:05:12,994 - data_ingestion - INFO - Database engine created successfully.\n",
      "2024-03-01 18:05:12,998 - data_ingestion - ERROR - The query returned an empty DataFrame.\n",
      "2024-03-01 18:05:12,998 - data_ingestion - ERROR - SQL query failed. Error: The query returned an empty DataFrame.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The query returned an empty DataFrame.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 13\u001b[0m\n\u001b[0;32m      3\u001b[0m sql_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124mSELECT *\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124mFROM geographic_features\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124mWHERE Rainfall < 0 \u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# The last line won't ever be true, so no results will be returned. \u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m df \u001b[38;5;241m=\u001b[39m query_data(SQL_engine, sql_query)\n\u001b[0;32m     14\u001b[0m df\n",
      "Cell \u001b[1;32mIn[14], line 39\u001b[0m, in \u001b[0;36mquery_data\u001b[1;34m(engine, sql_query)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e: \n\u001b[0;32m     38\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSQL query failed. Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     41\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while querying the database. Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[14], line 34\u001b[0m, in \u001b[0;36mquery_data\u001b[1;34m(engine, sql_query)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m df\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m# Log a message or handle the empty DataFrame scenario as needed\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe query returned an empty DataFrame.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 34\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe query returned an empty DataFrame.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     35\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery executed successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[1;31mValueError\u001b[0m: The query returned an empty DataFrame."
     ]
    }
   ],
   "source": [
    "SQL_engine = create_db_engine('sqlite:///Maji_Ndogo_farm_survey_small.db')\n",
    "\n",
    "sql_query = \"\"\"\n",
    "SELECT *\n",
    "FROM geographic_features\n",
    "LEFT JOIN weather_features USING (Field_ID)\n",
    "LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "LEFT JOIN farm_management_features USING (Field_ID)\n",
    "WHERE Rainfall < 0 \n",
    "\"\"\"\n",
    "# The last line won't ever be true, so no results will be returned. \n",
    "\n",
    "df = query_data(SQL_engine, sql_query)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca56a965",
   "metadata": {},
   "source": [
    "Next up, let's include the CSV data handling. This is the original code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5457a12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-01 18:12:25,897 - data_ingestion - INFO - CSV file read successfully from the web.\n",
      "2024-03-01 18:12:26,725 - data_ingestion - INFO - CSV file read successfully from the web.\n"
     ]
    }
   ],
   "source": [
    "weather_data_URL = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\"\n",
    "weather_mapping_data_URL = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\"\n",
    "\n",
    "\n",
    "def read_from_web_CSV(URL):\n",
    "    try:\n",
    "        df = pd.read_csv(URL)\n",
    "        logger.info(\"CSV file read successfully from the web.\")\n",
    "        return df\n",
    "    except pd.errors.EmptyDataError as e:\n",
    "        logger.error(\"The URL does not point to a valid CSV file. Please check the URL and try again.\")\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to read CSV from the web. Error: {e}\")\n",
    "        raise e\n",
    "    \n",
    "    \n",
    "weather_df = read_from_web_CSV(weather_data_URL)\n",
    "weather_mapping_data = read_from_web_CSV(weather_mapping_data_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c104ae30",
   "metadata": {},
   "source": [
    "Great! Now our code can connect to a database for the field data, use a query to retrieve data and create a DataFrame. We can also import CSV files from a URL into a DataFrame, and avoid pulling unexpected data. If we put all this code together we have the basic structure of our module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd844692",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sqlalchemy import create_engine, text\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "# Name our logger so we know that logs from this module come from the data_ingestion module\n",
    "logger = logging.getLogger('data_ingestion')\n",
    "# Set a basic logging message up that prints out a timestamp, the name of our logger, and the message\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "db_path = 'sqlite:///Maji_Ndogo_farm_survey_small.db'\n",
    "\n",
    "sql_query = \"\"\"\n",
    "SELECT *\n",
    "FROM geographic_features\n",
    "LEFT JOIN weather_features USING (Field_ID)\n",
    "LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "LEFT JOIN farm_management_features USING (Field_ID)\n",
    "\"\"\"\n",
    "\n",
    "weather_data_URL = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\"\n",
    "weather_mapping_data_URL = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\"\n",
    "\n",
    "def create_db_engine(db_path):\n",
    "    try:\n",
    "        engine = create_engine(db_path)\n",
    "        # Test connection\n",
    "        with engine.connect() as conn:\n",
    "            pass\n",
    "        # test if the database engine was created successfully\n",
    "        logger.info(\"Database engine created successfully.\")\n",
    "        return engine # Return the engine object if it all works well\n",
    "    except ImportError: #If we get an ImportError, inform the user SQLAlchemy is not installed\n",
    "        logger.error(\"SQLAlchemy is required to use this function. Please install it first.\")\n",
    "        raise e\n",
    "    except Exception as e:# If we fail to create an engine inform the user\n",
    "        logger.error(f\"Failed to create database engine. Error: {e}\")\n",
    "        raise e\n",
    "    \n",
    "def query_data(engine, sql_query):\n",
    "    try:\n",
    "        with engine.connect() as connection:\n",
    "            df = pd.read_sql_query(text(sql_query), connection)\n",
    "        if df.empty:\n",
    "            # Log a message or handle the empty DataFrame scenario as needed\n",
    "            msg = \"The query returned an empty DataFrame.\"\n",
    "            logger.error(msg)\n",
    "            raise ValueError(msg)\n",
    "        logger.info(\"Query executed successfully.\")\n",
    "        return df\n",
    "    except ValueError as e: \n",
    "        logger.error(f\"SQL query failed. Error: {e}\")\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred while querying the database. Error: {e}\")\n",
    "        raise e\n",
    "    \n",
    "def read_from_web_CSV(URL):\n",
    "    try:\n",
    "        df = pd.read_csv(URL)\n",
    "        logger.info(\"CSV file read successfully from the web.\")\n",
    "        return df\n",
    "    except pd.errors.EmptyDataError as e:\n",
    "        logger.error(\"The URL does not point to a valid CSV file. Please check the URL and try again.\")\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to read CSV from the web. Error: {e}\")\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9f9e0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-01 18:13:51,424 - data_ingestion - INFO - Database engine created successfully.\n",
      "2024-03-01 18:13:51,493 - data_ingestion - INFO - Query executed successfully.\n",
      "2024-03-01 18:13:52,257 - data_ingestion - INFO - CSV file read successfully from the web.\n",
      "2024-03-01 18:13:52,945 - data_ingestion - INFO - CSV file read successfully from the web.\n"
     ]
    }
   ],
   "source": [
    "# Testing module functions  \n",
    "field_df = query_data(create_db_engine(db_path), sql_query)   \n",
    "weather_df = read_from_web_CSV(weather_data_URL)\n",
    "weather_mapping_df = read_from_web_CSV(weather_mapping_data_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88434ce",
   "metadata": {},
   "source": [
    "## Field data processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36318303",
   "metadata": {},
   "source": [
    "Next up, let's process the field data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfa03de",
   "metadata": {},
   "source": [
    "Here our approach needs to be a bit different. If we create a module with a bunch of functions, we are going to have a hard time moving the DataFrame around the whole time. Instead, we're going to build a Class that encapsulates the whole data processing process for the field-related data called `FieldDataProcessor`. In the class, we will create a DataFrame attribute and methods that alter that attribute. So we encapsulate all of the logic in this `FieldDataProcessor` class, we abstract all of the details and only need to call something like `FieldDataProcessor.process_data()`. \n",
    "\n",
    "We could even include Inheritance and Polymorphism if we create a `DataProcessor` super class and create subclasses for `FieldDataProcessor` and `WeatherDataProcessor`.  But, there is a good reason not to. The data handling of the field data is quite different from the handling of the weather data. The field data comes from an SQL database, and we transform the data in a particular way, while the weather data is sourced from a CSV, and processed differently.\n",
    "\n",
    "So these two processes don't share processing steps, so it makes more sense to make a class for each.\n",
    "\n",
    "So let's create the class framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "419f45c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from data_procrssing.data_ingestion import create_db_engine, query_data, read_from_web_CSV\n",
    "import logging\n",
    "\n",
    "class FieldDataProcessor:\n",
    "    \n",
    "    def __init__(self, logging_level=\"INFO\"): # When we instantiate this class, we can optionally specify what logs we want to see\n",
    "       \n",
    "        # Initialising class with attributes we need. Refer to the code above to understand how each attribute relates to the code\n",
    "        self.db_path = 'sqlite:///Maji_Ndogo_farm_survey_small.db'\n",
    "        self.sql_query = \"\"\"\n",
    "            SELECT *\n",
    "            FROM geographic_features\n",
    "            LEFT JOIN weather_features USING (Field_ID)\n",
    "            LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "            LEFT JOIN farm_management_features USING (Field_ID)\n",
    "            \"\"\"\n",
    "        self.columns_to_rename = {'Annual_yield': 'Crop_type', 'Crop_type': 'Annual_yield'}\n",
    "        self.values_to_rename = {'cassaval': 'cassava', 'wheatn': 'wheat', 'teaa': 'tea'}\n",
    "        self.weather_map_data = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\"\n",
    "\n",
    "        self.initialize_logging(logging_level)\n",
    "        \n",
    "        # We create empty objects to store the DataFrame and engine in\n",
    "        self.df = None\n",
    "        self.engine = None\n",
    "        \n",
    "    # This method enables logging in the class. \n",
    "    def initialize_logging(self, logging_level):\n",
    "\n",
    "        logger_name = __name__ + \".FieldDataProcessor\"\n",
    "        self.logger = logging.getLogger(logger_name)\n",
    "        self.logger.propagate = False  # Prevents log messages from being propagated to the root logger\n",
    "\n",
    "        # Set logging level\n",
    "        if logging_level.upper() == \"DEBUG\":\n",
    "            log_level = logging.DEBUG\n",
    "        elif logging_level.upper() == \"INFO\":\n",
    "            log_level = logging.INFO\n",
    "        elif logging_level.upper() == \"NONE\":  # Option to disable logging\n",
    "            self.logger.disabled = True\n",
    "            return\n",
    "        else:\n",
    "            log_level = logging.INFO  # Default to INFO\n",
    "\n",
    "        self.logger.setLevel(log_level)\n",
    "\n",
    "        # Only add handler if not already added to avoid duplicate messages\n",
    "        if not self.logger.handlers:\n",
    "            ch = logging.StreamHandler()  # Create console handler\n",
    "            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "            ch.setFormatter(formatter)\n",
    "            self.logger.addHandler(ch)\n",
    "\n",
    "        # Use self.logger.info(), self.logger.debug(), etc.\n",
    "\n",
    "\n",
    "    # DataFrame methods \n",
    "    def ingest_sql_data(self):\n",
    "        # First we want to get the data from the SQL database\n",
    "        pass\n",
    "    \n",
    "    def rename_columns(self):\n",
    "        # Annual_yield and Crop_type must be swapped\n",
    "        pass\n",
    "\n",
    "    def apply_corrections(self):\n",
    "        # Correct the crop strings, Eg: 'cassaval' -> 'cassava'\n",
    "        pass\n",
    "\n",
    "    def weather_station_mapping(self):\n",
    "        # Merge the weather station data to the main DataFrame\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        # This process calls the correct methods and applies the changes, step by step. This is the method we will call, and it will call the other methods in order\n",
    "        \n",
    "        weather_map_df = self.weather_station_mapping() \n",
    "        self.df = self.ingest_sql_data()\n",
    "        self.df = self.rename_columns()\n",
    "        self.df = self.apply_corrections()\n",
    "        self.df = self.df.merge(weather_map_df, on='Field_ID', how='left')\n",
    "        self.df = self.df.drop(columns=\"Unnamed: 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492c5e25",
   "metadata": {},
   "source": [
    "now we need to develop the class methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e6f89a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from data_procrssing.data_ingestion import create_db_engine, query_data, read_from_web_CSV\n",
    "import logging\n",
    "\n",
    "\n",
    "class FieldDataProcessor:\n",
    "   \n",
    "    def __init__(self, config_params, logging_level=\"INFO\"):  # Make sure to add this line, passing in config_params to the class \n",
    "        self.db_path = config_params['db_path']\n",
    "        self.sql_query = config_params['sql_query']\n",
    "        self.columns_to_rename = config_params['columns_to_rename']\n",
    "        self.values_to_rename = config_params['values_to_rename']\n",
    "        self.weather_map_data = config_params['weather_mapping_csv']\n",
    "        \n",
    "        self.initialize_logging(logging_level)\n",
    "\n",
    "        # We create empty objects to store the DataFrame and engine in\n",
    "        self.df = None\n",
    "        self.engine = None\n",
    "    \n",
    "\n",
    "    # This method enables logging in the class.\n",
    "    def initialize_logging(self, logging_level):        \n",
    "        logger_name = __name__ + \".FieldDataProcessor\"\n",
    "        self.logger = logging.getLogger(logger_name)\n",
    "        self.logger.propagate = False  # Prevents log messages from being propagated to the root logger\n",
    "\n",
    "        # Set logging level\n",
    "        if logging_level.upper() == \"DEBUG\":\n",
    "            log_level = logging.DEBUG\n",
    "        elif logging_level.upper() == \"INFO\":\n",
    "            log_level = logging.INFO\n",
    "        elif logging_level.upper() == \"NONE\":  # Option to disable logging\n",
    "            self.logger.disabled = True\n",
    "            return\n",
    "        else:\n",
    "            log_level = logging.INFO  # Default to INFO\n",
    "\n",
    "        self.logger.setLevel(log_level)\n",
    "\n",
    "        # Only add handler if not already added to avoid duplicate messages\n",
    "        if not self.logger.handlers:\n",
    "            ch = logging.StreamHandler()  # Create console handler\n",
    "            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "            ch.setFormatter(formatter)\n",
    "            self.logger.addHandler(ch)\n",
    "\n",
    "        # Use self.logger.info(), self.logger.debug(), etc.\n",
    "            \n",
    "\n",
    "    # let's focus only on this part from now on\n",
    "    def ingest_sql_data(self):\n",
    "        self.engine = create_db_engine(self.db_path)\n",
    "        self.df = query_data(self.engine, self.sql_query)\n",
    "        self.logger.info(\"Sucessfully loaded data.\")\n",
    "        return self.df\n",
    "\n",
    "\n",
    "    def rename_columns(self):\n",
    "        # Extract the columns to rename from the configuration\n",
    "        column1, column2 = list(self.columns_to_rename.keys())[0], list(self.columns_to_rename.values())[0]       \n",
    "        # Temporarily rename one of the columns to avoid a naming conflict\n",
    "        temp_name = \"__temp_name_for_swap__\"\n",
    "        while temp_name in self.df.columns:\n",
    "            temp_name += \"_\"\n",
    "        # Perform the swap\n",
    "        self.df = self.df.rename(columns={column1: temp_name, column2: column1})\n",
    "        self.df = self.df.rename(columns={temp_name: column2})\n",
    "        self.logger.info(f\"Swapped columns: {column1} with {column2}\")\n",
    "        return self.df\n",
    "        \n",
    "            \n",
    "    def apply_corrections(self, column_name='Crop_type', abs_column='Elevation'):\n",
    "        self.df[abs_column] = self.df[abs_column].abs()\n",
    "        self.df[column_name] = self.df[column_name].apply(lambda crop: self.values_to_rename.get(crop, crop))\n",
    "        return self.df\n",
    "\n",
    "    \n",
    "    def weather_station_mapping(self):\n",
    "        return read_from_web_CSV(self.weather_map_data)\n",
    "    \n",
    "    \n",
    "    def process(self):\n",
    "        self.df = self.ingest_sql_data()\n",
    "        self.df = self.rename_columns()\n",
    "        self.df = self.apply_corrections()\n",
    "        weather_map_df = self.weather_station_mapping() \n",
    "        self.df = self.df.merge(weather_map_df, on='Field_ID', how='left')\n",
    "        self.df = self.df.drop(columns=\"Unnamed: 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d745fd2",
   "metadata": {},
   "source": [
    "Here is how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4160991",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-01 21:27:45,032 - data_ingestion - INFO - Database engine created successfully.\n",
      "2024-03-01 21:27:45,104 - data_ingestion - INFO - Query executed successfully.\n",
      "2024-03-01 21:27:45,104 - __main__.FieldDataProcessor - INFO - Sucessfully loaded data.\n",
      "2024-03-01 21:27:45,109 - __main__.FieldDataProcessor - INFO - Swapped columns: Annual_yield with Crop_type\n",
      "2024-03-01 21:27:45,804 - data_ingestion - INFO - CSV file read successfully from the web.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4, 0, 1, 2, 3], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_params = {\n",
    "    \"sql_query\": \"\"\"\n",
    "                SELECT *\n",
    "                FROM geographic_features\n",
    "                LEFT JOIN weather_features USING (Field_ID)\n",
    "                LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "                LEFT JOIN farm_management_features USING (Field_ID)\n",
    "            \"\"\",\n",
    "    \"db_path\": 'sqlite:///Maji_Ndogo_farm_survey_small.db', # Insert the db_path of the database\n",
    "    \"columns_to_rename\": {'Annual_yield': 'Crop_type', 'Crop_type': 'Annual_yield'},# Insert the disctionary of columns we want to swop the names of, \n",
    "    \"values_to_rename\": {'cassaval': 'cassava', 'wheatn': 'wheat', 'teaa': 'tea'}, # Insert the croptype renaming dictionary\n",
    "    \"weather_csv_path\": \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\", # Insert the weather data CSV here\n",
    "    \"weather_mapping_csv\":\"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\", # Insert the weather data mapping CSV here\n",
    "}\n",
    "\n",
    "field_processor = FieldDataProcessor(config_params)\n",
    "field_processor.process()\n",
    "\n",
    "field_df = field_processor.df\n",
    "field_df['Weather_station'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c39e19",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "### Creating `field_data_processor.py`\n",
    "\n",
    "Now we have a robust data processing class for the field-related data. Our final step is to create the module file and document the code.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8bc644",
   "metadata": {},
   "source": [
    "## Weather data processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c18e210",
   "metadata": {},
   "source": [
    "Now for the last module. The `WeatherDataProcessor` class will be dealing with all of the weather-related data. Again we want to instantiate the class, then call a `.process()` method to import and clean the data. Here is the code we used last time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b1a87348",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "from data_procrssing.data_ingestion import read_from_web_CSV\n",
    "\n",
    "\n",
    "class WeatherDataProcessor:\n",
    "    \"\"\"\n",
    "    A class for processing weather station data.\n",
    "\n",
    "    Parameters:\n",
    "    - config_params (dict): Configuration parameters for data processing.\n",
    "    - logging_level (str, optional): Logging level for the class (default is \"INFO\").\n",
    "\n",
    "    Attributes:\n",
    "    - weather_station_data (str): CSV file path for weather station data.\n",
    "    - patterns (dict): Regular expression patterns for extracting measurements from messages.\n",
    "    - weather_df (pd.DataFrame): DataFrame to store weather station data.\n",
    "    - logger (logging.Logger): Logger object for logging messages.\n",
    "\n",
    "    Methods:\n",
    "    - initialize_logging(logging_level): Set up logging for the instance.\n",
    "    - weather_station_mapping(): Load weather station data from the web.\n",
    "    - extract_measurement(message): Extract measurements from a given message using regex patterns.\n",
    "    - process_messages(): Process messages in the DataFrame to extract measurements.\n",
    "    - calculate_means(): Calculate mean values for each weather station and measurement.\n",
    "    - process(): Execute all methods in the correct order for data processing.\n",
    "    \"\"\"\n",
    "    def __init__(self, config_params, logging_level=\"INFO\"): # Now we're passing in the confi_params dictionary already\n",
    "        \"\"\"\n",
    "        Initialize the WeatherDataProcessor instance.\n",
    "\n",
    "        Parameters:\n",
    "        - config_params (dict): Configuration parameters for data processing.\n",
    "        - logging_level (str, optional): Logging level for the class (default is \"INFO\").\n",
    "        \"\"\"\n",
    "        self.weather_station_data = config_params['weather_csv_path']\n",
    "        self.patterns = config_params['regex_patterns']\n",
    "        self.weather_df = None  # Initialize weather_df as None or as an empty DataFrame\n",
    "        self.initialize_logging(logging_level)\n",
    "\n",
    "    def initialize_logging(self, logging_level):\n",
    "        \"\"\"\n",
    "        Set up logging for this instance of WeatherDataProcessor.\n",
    "\n",
    "        Parameters:\n",
    "        - logging_level (str): Logging level for the class.\n",
    "        \"\"\"\n",
    "        logger_name = __name__ + \".WeatherDataProcessor\"\n",
    "        self.logger = logging.getLogger(logger_name)\n",
    "        self.logger.propagate = False  # Prevents log messages from being propagated to the root logger\n",
    "\n",
    "        # Set logging level\n",
    "        if logging_level.upper() == \"DEBUG\":\n",
    "            log_level = logging.DEBUG\n",
    "        elif logging_level.upper() == \"INFO\":\n",
    "            log_level = logging.INFO\n",
    "        elif logging_level.upper() == \"NONE\":  # Option to disable logging\n",
    "            self.logger.disabled = True\n",
    "            return\n",
    "        else:\n",
    "            log_level = logging.INFO  # Default to INFO\n",
    "\n",
    "        self.logger.setLevel(log_level)\n",
    "\n",
    "        # Only add handler if not already added to avoid duplicate messages\n",
    "        if not self.logger.handlers:\n",
    "            ch = logging.StreamHandler()  # Create console handler\n",
    "            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "            ch.setFormatter(formatter)\n",
    "            self.logger.addHandler(ch)\n",
    "\n",
    "    def weather_station_mapping(self):\n",
    "        \"\"\"\n",
    "        Load weather station data from the web.\n",
    "\n",
    "        Returns:\n",
    "        - pd.DataFrame: Weather station data DataFrame.\n",
    "        \"\"\"\n",
    "        self.weather_df = read_from_web_CSV(self.weather_station_data)\n",
    "        self.logger.info(\"Successfully loaded weather station data from the web.\") \n",
    "        # Here, you can apply any initial transformations to self.weather_df if necessary.\n",
    "\n",
    "    \n",
    "    def extract_measurement(self, message):\n",
    "        \"\"\"\n",
    "        Extract measurements from a given message using regex patterns.\n",
    "\n",
    "        Parameters:\n",
    "        - message (str): The message containing weather measurements.\n",
    "\n",
    "        Returns:\n",
    "        - tuple or None: A tuple containing the measurement key and value, or None if no match is found.\n",
    "        \"\"\"\n",
    "        for key, pattern in self.patterns.items():\n",
    "            match = re.search(pattern, message)\n",
    "            if match:\n",
    "                self.logger.debug(f\"Measurement extracted: {key}\")\n",
    "                return key, float(next((x for x in match.groups() if x is not None)))\n",
    "        self.logger.debug(\"No measurement match found.\")\n",
    "        return None, None\n",
    "\n",
    "    def process_messages(self):\n",
    "        \"\"\"\n",
    "        Process messages in the DataFrame to extract measurements.\n",
    "\n",
    "        Returns:\n",
    "        - pd.DataFrame: Processed DataFrame with extracted measurements.\n",
    "        \"\"\"\n",
    "        if self.weather_df is not None:\n",
    "            result = self.weather_df['Message'].apply(self.extract_measurement)\n",
    "            self.weather_df['Measurement'], self.weather_df['Value'] = zip(*result)\n",
    "            self.logger.info(\"Messages processed and measurements extracted.\")\n",
    "        else:\n",
    "            self.logger.warning(\"weather_df is not initialized, skipping message processing.\")\n",
    "        return self.weather_df\n",
    "\n",
    "    def calculate_means(self):\n",
    "        \"\"\"\n",
    "        Calculate mean values for each weather station and measurement.\n",
    "\n",
    "        Returns:\n",
    "        - pd.DataFrame or None: DataFrame with mean values or None if weather_df is not initialized.\n",
    "        \"\"\"\n",
    "        if self.weather_df is not None:\n",
    "            means = self.weather_df.groupby(by=['Weather_station_ID', 'Measurement'])['Value'].mean()\n",
    "            self.logger.info(\"Mean values calculated.\")\n",
    "            return means.unstack()\n",
    "        else:\n",
    "            self.logger.warning(\"weather_df is not initialized, cannot calculate means.\")\n",
    "            return None\n",
    "    \n",
    "    def process(self):\n",
    "        \"\"\"\n",
    "        Execute all methods in the correct order for data processing.\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        self.weather_station_mapping()  # Load and assign data to weather_df\n",
    "        self.process_messages()  # Process messages to extract measurements\n",
    "        self.logger.info(\"Data processing completed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203cfbc9",
   "metadata": {},
   "source": [
    "here is an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22f7bb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-01 22:01:51,394 - data_ingestion - INFO - CSV file read successfully from the web.\n",
      "2024-03-01 22:01:51,394 - data_procrssing.weather_data_processor.WeatherDataProcessor - INFO - Successfully loaded weather station data from the web.\n",
      "2024-03-01 22:01:51,454 - data_procrssing.weather_data_processor.WeatherDataProcessor - INFO - Messages processed and measurements extracted.\n",
      "2024-03-01 22:01:51,455 - data_procrssing.weather_data_processor.WeatherDataProcessor - INFO - Data processing completed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Temperature', 'Pollution_level', 'Rainfall'], dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from field_data_processor import FieldDataProcessor\n",
    "from data_procrssing.weather_data_processor import WeatherDataProcessor\n",
    "import logging \n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "config_params = {\n",
    "    # Paste in your previous dictionary data in here\n",
    "     \"sql_query\": \"\"\"\n",
    "                SELECT *\n",
    "                FROM geographic_features\n",
    "                LEFT JOIN weather_features USING (Field_ID)\n",
    "                LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "                LEFT JOIN farm_management_features USING (Field_ID)\n",
    "            \"\"\", # Insert your SQL query\n",
    "    \"db_path\": 'sqlite:///Maji_Ndogo_farm_survey_small.db', # Insert the db_path of the database\n",
    "    \"columns_to_rename\": {'Annual_yield': 'Crop_type', 'Crop_type': 'Annual_yield'},# Insert the disctionary of columns we want to swop the names of, \n",
    "    \"values_to_rename\": {'cassaval': 'cassava', 'wheatn': 'wheat', 'teaa': 'tea'}, # Insert the croptype renaming dictionary\n",
    "    \"weather_csv_path\": \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\", # Insert the weather data CSV here\n",
    "    \"weather_mapping_csv\":\"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\", # Insert the weather data mapping CSV here\n",
    "\n",
    "    # Add two new keys\n",
    "    \"regex_patterns\" :  {\n",
    "    'Rainfall': r'(\\d+(\\.\\d+)?)\\s?mm',\n",
    "     'Temperature': r'(\\d+(\\.\\d+)?)\\s?C',\n",
    "    'Pollution_level': r'=\\s*(-?\\d+(\\.\\d+)?)|Pollution at \\s*(-?\\d+(\\.\\d+)?)'\n",
    "    } # Insert the regex pattern we used to process the messages\n",
    "}\n",
    "# Ignoring the field data for now.\n",
    "# field_processor = FieldDataProcessor(config_params)\n",
    "# field_processor.process()\n",
    "# field_df = field_processor.df\n",
    "\n",
    "weather_processor = WeatherDataProcessor(config_params)\n",
    "weather_processor.process()\n",
    "weather_df = weather_processor.weather_df\n",
    "\n",
    "weather_df['Measurement'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c869aacc",
   "metadata": {},
   "source": [
    "### Validating our data pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b729ea11",
   "metadata": {},
   "source": [
    "So we finally have working modules that now automatically pull data from the database  (or the web), process it, clean it, and return our starting DataFrame. Before we jump in and analyse the data, let's pause for a second and ask: Did the changes actually get applied? Did we correct the elevation data, did we rename the columns? We could go back to the old ways, and create queries to check, but a better way is to **test our dataset**. \n",
    "\n",
    "Let's get the data in first. Remember to use your `config_params` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7ba609f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-01 22:16:22,214 - data_ingestion - INFO - Database engine created successfully.\n",
      "2024-03-01 22:16:22,284 - data_ingestion - INFO - Query executed successfully.\n",
      "2024-03-01 22:16:22,284 - data_procrssing.field_data_processor.FieldDataProcessor - INFO - Sucessfully loaded data.\n",
      "2024-03-01 22:16:22,287 - data_procrssing.field_data_processor.FieldDataProcessor - INFO - Swapped columns: Annual_yield with Crop_type\n",
      "2024-03-01 22:16:23,037 - data_ingestion - INFO - CSV file read successfully from the web.\n",
      "2024-03-01 22:16:24,208 - data_ingestion - INFO - CSV file read successfully from the web.\n",
      "2024-03-01 22:16:24,209 - data_procrssing.weather_data_processor.WeatherDataProcessor - INFO - Successfully loaded weather station data from the web.\n",
      "2024-03-01 22:16:24,228 - data_procrssing.weather_data_processor.WeatherDataProcessor - INFO - Messages processed and measurements extracted.\n",
      "2024-03-01 22:16:24,229 - data_procrssing.weather_data_processor.WeatherDataProcessor - INFO - Data processing completed.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from data_procrssing.field_data_processor import FieldDataProcessor\n",
    "from data_procrssing.weather_data_processor import WeatherDataProcessor\n",
    "import logging \n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "config_params = {\n",
    "     \"sql_query\": \"\"\"\n",
    "                SELECT *\n",
    "                FROM geographic_features\n",
    "                LEFT JOIN weather_features USING (Field_ID)\n",
    "                LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "                LEFT JOIN farm_management_features USING (Field_ID)\n",
    "            \"\"\",\n",
    "    \"db_path\": 'sqlite:///Maji_Ndogo_farm_survey_small.db',\n",
    "    \"columns_to_rename\": {'Annual_yield': 'Crop_type', 'Crop_type': 'Annual_yield'},\n",
    "    \"values_to_rename\": {'cassaval': 'cassava', 'wheatn': 'wheat', 'teaa': 'tea'},\n",
    "    \"weather_csv_path\": \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\",\n",
    "    \"weather_mapping_csv\":\"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\",\n",
    "    \"regex_patterns\" :  {\n",
    "    'Rainfall': r'(\\d+(\\.\\d+)?)\\s?mm',\n",
    "     'Temperature': r'(\\d+(\\.\\d+)?)\\s?C',\n",
    "    'Pollution_level': r'=\\s*(-?\\d+(\\.\\d+)?)|Pollution at \\s*(-?\\d+(\\.\\d+)?)'\n",
    "    }\n",
    "}\n",
    "\n",
    "field_processor = FieldDataProcessor(config_params)\n",
    "field_processor.process()\n",
    "field_df = field_processor.df\n",
    "\n",
    "weather_processor = WeatherDataProcessor(config_params)\n",
    "weather_processor.process()\n",
    "weather_df = weather_processor.weather_df\n",
    "\n",
    "# Rename 'Ave_temps' in field_df to 'Temperature' to match weather_df\n",
    "field_df.rename(columns={'Ave_temps': 'Temperature'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b693f68e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data_Science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
