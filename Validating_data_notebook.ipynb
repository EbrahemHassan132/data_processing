{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5371cda3",
   "metadata": {},
   "source": [
    "# Validating data notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f418367",
   "metadata": {},
   "source": [
    "#### In this notebook is a demonesrtaion of how to validate data using another data source\n",
    "\n",
    "This is done using a data `pipeline` that will ingest and clean our data with the press of a button"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88e725a",
   "metadata": {},
   "source": [
    "we will use a dataset from an imaginary country called `Maji Ndogo` created by `Explore-AI` academy\n",
    "the dataset is containing agricultural data about the fields of `Maji Ndogo`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc719458",
   "metadata": {},
   "source": [
    "the dataset contains a wheater data and we will validate these data using a data collected from a weather stations in the area of each field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1548521f",
   "metadata": {},
   "source": [
    "Our main goal is: Is the data in our `MD_agric_df` dataset representative of reality? To answer this, we use weather-related data from nearby stations to validate our results. If the weather data matches the data we have, we can be more confident that our dataset represents reality. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607ba18a",
   "metadata": {},
   "source": [
    "So what's the plan? \n",
    "1. Create a null hypothesis.\n",
    "1. Import the `MD_agric_df` dataset and clean it up.\n",
    "1. Import the weather data.\n",
    "1. Map the weather data to the field data.\n",
    "1. Calculate the means of the weather station dataset and the means of the main dataset.\n",
    "2. Calculate all the parameters we need to do a t-test. \n",
    "3. Interpret our results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c47bb5",
   "metadata": {},
   "source": [
    "# Data dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f3cfa1",
   "metadata": {},
   "source": [
    "**1. Geographic features**\n",
    "\n",
    "- **Field_ID:** A unique identifier for each field (BigInt).\n",
    " \n",
    "- **Elevation:** The elevation of the field above sea level in metres (Float).\n",
    "\n",
    "- **Latitude:** Geographical latitude of the field in degrees (Float).\n",
    "\n",
    "- **Longitude:** Geographical longitude of the field in degrees (Float).\n",
    "\n",
    "- **Location:** Province the field is in (Text).\n",
    "\n",
    "- **Slope:** The slope of the land in the field (Float).\n",
    "\n",
    "**2. Weather features**\n",
    "\n",
    "- **Field_ID:** Corresponding field identifier (BigInt).\n",
    "\n",
    "- **Rainfall:** Amount of rainfall in the area in mm (Float).\n",
    "\n",
    "- **Min_temperature_C:** Average minimum temperature recorded in Celsius (Float).\n",
    "\n",
    "- **Max_temperature_C:** Average maximum temperature recorded in Celsius (Float).\n",
    "\n",
    "- **Ave_temps:** Average temperature in Celcius (Float).\n",
    "\n",
    "**3. Soil and crop features**\n",
    "\n",
    "- **Field_ID:** Corresponding field identifier (BigInt).\n",
    "\n",
    "- **Soil_fertility:** A measure of soil fertility where 0 is infertile soil, and 1 is very fertile soil (Float).\n",
    "\n",
    "- **Soil_type:** Type of soil present in the field (Text).\n",
    "\n",
    "- **pH:** pH level of the soil, which is a measure of how acidic/basic the soil is (Float).\n",
    "\n",
    "**4. Farm management features**\n",
    "\n",
    "- **Field_ID:** Corresponding field identifier (BigInt).\n",
    "\n",
    "- **Pollution_level:** Level of pollution in the area where 0 is unpolluted and 1 is very polluted (Float).\n",
    "\n",
    "- **Plot_size:** Size of the plot in the field (Ha) (Float).\n",
    "\n",
    "- **Chosen_crop:** Type of crop chosen for cultivation (Text).\n",
    "\n",
    "- **Annual_yield:** Annual yield from the field (Float). This is the total output of the field. The field size and type of crop will affect the Annual Yield\n",
    "\n",
    "- **Standard_yield:** Standardised yield expected from the field, normalised per crop (Float). This is independent of field size, or crop type. Multiplying this number by the field size, and average crop yield will give the Annual_Yield.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Weather_station_data (CSV)**\n",
    "\n",
    "- **Weather_station_ID:** The weather station the data originated from. (Int)\n",
    "\n",
    "- **Message:** The weather data was captured by sensors at the stations, in the format of text messages.(Str)\n",
    "\n",
    "**Weather_data_field_mapping (CSV)**\n",
    "\n",
    "- **Field_ID:** The id of the field that is connected to a weather station. This is the key we can use to join the weather station ID to the original data. (Int)\n",
    "\n",
    "- **Weather_station_ID:** The weather station that is connected to a field. If a field has `weather_station_ID = 0` then that field is closest to weather station 0. (Int)\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7413a4",
   "metadata": {},
   "source": [
    "# Dealing with a friendly warning from Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6862b309",
   "metadata": {},
   "source": [
    "If you are running this notebook in `Python 3.12` or later, you might get a warning if you run the imports below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ff4521",
   "metadata": {},
   "source": [
    "If you are lucky enough to see this warning, it let's us know that Pandas is changing soon and will require another package to be installed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b6ba10",
   "metadata": {},
   "source": [
    "```python\n",
    "...2334042735.py:3: DeprecationWarning: \n",
    "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
    "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
    "but was not found to be installed on your system.\n",
    "If this would cause problems for you,\n",
    "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
    "        \n",
    "  import pandas as pd\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f0febb",
   "metadata": {},
   "source": [
    "We can safely ignore these warnings, but soon our script will fail to import Pandas, so let's fix it today, and we won't have to worry about it for a long time. The warning tells us that Pyarrow will soon be a requirement to import Pandas, so we can just install it with pip. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064d9f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install Pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b715617",
   "metadata": {},
   "source": [
    "# Creating up our data pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a427b29",
   "metadata": {},
   "source": [
    "So here's the plan: \n",
    "\n",
    "1. Create a module for each step.\n",
    "\n",
    "2. We will create three modules: \n",
    "\n",
    "    a. `data_ingesation.py` - All SQL-related functions, and web-based data retrieval.\n",
    "\n",
    "    b. `field_data_processor.py` - All transformations, cleanup, and merging functionality.\n",
    "\n",
    "    c. `weather_data_processor.py` - All transformations and cleanup of the weather station data.\n",
    "\n",
    "3. Test the modules functionality.\n",
    "\n",
    "4. Create automated data validation tests to ensure our data is as we expect it to be.\n",
    "\n",
    "Once we're done with that, we're going to jump into the reason why we're here. So let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffa809d",
   "metadata": {},
   "source": [
    "The first challenge; automating the data ingestion. There are two places we're fetching data:\n",
    "1. SQLite database - We need to create an SQLite engine, connect to the database, run a query and return a pandas DataFrame.\n",
    "2. Web CSV file - Read the CSV data from the web, and import it as a DataFrame.\n",
    "\n",
    "So let's start building!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3401e792",
   "metadata": {},
   "source": [
    "## Data ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646f0f29",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
